{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FYp6WTHIHJ-",
        "outputId": "2841e69d-b311-46ea-b3c0-fdd4cf7009d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Load pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5m-Qq2N6IIca"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.read()\n",
        "        tokenized_text = tokenizer.encode(lines)\n",
        "        self.examples = [tokenized_text[i:i + block_size] for i in range(0, len(tokenized_text) - block_size + 1, block_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pGBjusc7JlP_"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(file_path='custom_dataset.txt', tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "KKIOZLPfJoPV",
        "outputId": "d3bec017-5539-472b-edeb-fe4a879e4e6d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:31, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.451494598388672, metrics={'train_runtime': 31.9749, 'train_samples_per_second': 1.251, 'train_steps_per_second': 0.313, 'total_flos': 9287006945280.0, 'train_loss': 2.451494598388672, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-finetuned',  # Output directory\n",
        "    overwrite_output_dir=True,      # Overwrite the content of the output directory\n",
        "    num_train_epochs=10,             # Number of training epochs 3\n",
        "    per_device_train_batch_size=4,  # Batch size per GPU 2\n",
        "    save_steps=1000,              # Save checkpoint every 10,000 steps\n",
        "    save_total_limit=2,             # Limit the total amount of checkpoints\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "def data_collator(data):\n",
        "    return {\n",
        "        'input_ids': torch.stack([f for f in data]),\n",
        "        'labels': torch.stack([f for f in data]),\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G2u4N2eJqFE",
        "outputId": "6a64cdfe-1b2a-46bc-ceaf-0f12ef12c026"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-finetuned/tokenizer_config.json',\n",
              " './gpt2-finetuned/special_tokens_map.json',\n",
              " './gpt2-finetuned/vocab.json',\n",
              " './gpt2-finetuned/merges.txt',\n",
              " './gpt2-finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model.save_pretrained('./gpt2-finetuned')\n",
        "tokenizer.save_pretrained('./gpt2-finetuned')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_uoh4WGKUJG",
        "outputId": "6225275a-6cba-4718-f53b-6362801900ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Random forest is an ensemble learning Â method for classification problems.\n",
            "2: It uses a random subset of data and weights to classify the output, which means that it can be used in both supervised or unsupervised settings.\n",
            "3: Random Forest algorithm works by averaging over all possible input values (usually) then selecting one from among them, where each time step takes on average number\n",
            "randomForest algorithms are widely known for its ability at predicting outcomes based upon their distribution rather than simply using fixed value as inputs; this allows users more control when dealing with complex datasets such like image processing tasks etc.\n",
            "4: The main drawback of these methods is they tend not to perform well under noisy environments due mainly because there may only ever being enough training set available For example if you have 10 images\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "prompt = \"Random forest is an ensemble learning \"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda') # Move input_ids to the GPU\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,\n",
        "    top_p = 0.92,\n",
        "    temperature = 0.7,\n",
        "    repetition_penalty=1.2, # Penalize repetition\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "# print(generated_text)\n",
        "# Split the text into sentences based on periods\n",
        "sentences = re.split(r'(?<=\\.)\\s*', generated_text)\n",
        "\n",
        "# Print each sentence on a new line with numbering\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Grl9D2BENlyi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}